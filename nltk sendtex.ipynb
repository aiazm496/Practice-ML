{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"Hello, My name is Akash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', ',', 'is', 'it', 'raining', 'or', 'is', 'it', 'cloudy', '?']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(message)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = \"Hello!, My name is Akash. I work at IDeaS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello!, My name is Akash.', 'I work at IDeaS']"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para_tokens = nltk.sent_tokenize(para)\n",
    "para_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro = \"Hi Akash, How are you? It is raining here. The day temperature has significantly dropped. I am going to play STALKER at night. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi Akash, How are you?', 'It is raining here.', 'The day temperature has significantly dropped.', 'I am going to play STALKER at night.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(intro))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download_shell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi', 'NNP'), (',', ','), ('is', 'VBZ'), ('it', 'PRP'), ('raining', 'VBG'), ('or', 'CC'), ('is', 'VBZ'), ('it', 'PRP'), ('cloudy', 'NN'), ('?', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.pos_tag(tokens)) #nnp - nouns - vbz - verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Hi/NNP)\n",
      "  ,/,\n",
      "  is/VBZ\n",
      "  it/PRP\n",
      "  raining/VBG\n",
      "  or/CC\n",
      "  is/VBZ\n",
      "  it/PRP\n",
      "  cloudy/NN\n",
      "  ?/.)\n"
     ]
    }
   ],
   "source": [
    "print(nltk.ne_chunk(nltk.pos_tag(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'Akash', ',', 'How', 'are', 'you', '?', 'It', 'is', 'raining', 'here', '.', 'The', 'day', 'temperature', 'has', 'significantly', 'dropped', '.', 'I', 'am', 'going', 'to', 'play', 'STALKER', 'at', 'night', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(intro))  #treats punctuation as a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n",
      "Akash\n",
      ",\n",
      "How\n",
      "are\n",
      "you\n",
      "?\n",
      "It\n",
      "is\n",
      "raining\n",
      "here\n",
      ".\n",
      "The\n",
      "day\n",
      "temperature\n",
      "has\n",
      "significantly\n",
      "dropped\n",
      ".\n",
      "I\n",
      "am\n",
      "going\n",
      "to\n",
      "play\n",
      "STALKER\n",
      "at\n",
      "night\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for i in word_tokenize(intro):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        filtered_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'Akash',\n",
       " ',',\n",
       " 'How',\n",
       " '?',\n",
       " 'It',\n",
       " 'raining',\n",
       " '.',\n",
       " 'The',\n",
       " 'day',\n",
       " 'temperature',\n",
       " 'significantly',\n",
       " 'dropped',\n",
       " '.',\n",
       " 'I',\n",
       " 'going',\n",
       " 'play',\n",
       " 'STALKER',\n",
       " 'night',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8 stop words removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop words do not matter , so we remove them in pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer  #use stem to get parent word, used in search engines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n",
      "akash\n",
      ",\n",
      "how\n",
      "?\n",
      "It\n",
      "rain\n",
      ".\n",
      "the\n",
      "day\n",
      "temperatur\n",
      "significantli\n",
      "drop\n",
      ".\n",
      "I\n",
      "go\n",
      "play\n",
      "stalker\n",
      "night\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for word in filtered_words:\n",
    "    print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we could see it doesnt work well here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ask'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('asking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ask'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('asks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmetizing - similar word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer= WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> l\n",
      "\n",
      "Packages:\n",
      "  [ ] abc................. Australian Broadcasting Commission 2006\n",
      "  [ ] alpino.............. Alpino Dutch Treebank\n",
      "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
      "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
      "  [ ] basque_grammars..... Grammars for Basque\n",
      "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
      "                           Extraction Systems in Biology)\n",
      "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
      "  [ ] book_grammars....... Grammars from NLTK Book\n",
      "  [ ] brown............... Brown Corpus\n",
      "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
      "  [ ] cess_cat............ CESS-CAT Treebank\n",
      "  [ ] cess_esp............ CESS-ESP Treebank\n",
      "  [ ] chat80.............. Chat-80 Data Files\n",
      "  [ ] city_database....... City Database\n",
      "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
      "  [ ] comparative_sentences Comparative Sentence Dataset\n",
      "  [ ] comtrans............ ComTrans Corpus Sample\n",
      "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
      "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
      "Hit Enter to continue: \n",
      "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
      "                           and Basque Subset)\n",
      "  [ ] crubadan............ Crubadan Corpus\n",
      "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
      "  [ ] dolch............... Dolch Word List\n",
      "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
      "                           Corpus\n",
      "  [ ] floresta............ Portuguese Treebank\n",
      "  [ ] framenet_v15........ FrameNet 1.5\n",
      "  [ ] framenet_v17........ FrameNet 1.7\n",
      "  [ ] gazetteers.......... Gazeteer Lists\n",
      "  [ ] genesis............. Genesis Corpus\n",
      "  [ ] gutenberg........... Project Gutenberg Selections\n",
      "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
      "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
      "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
      "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
      "                           ChaSen format)\n",
      "  [ ] kimmo............... PC-KIMMO Data Files\n",
      "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
      "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
      "                           for parser comparison\n",
      "Hit Enter to continue: \n",
      "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
      "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
      "                           part-of-speech tags\n",
      "  [ ] machado............. Machado de Assis -- Obra Completa\n",
      "  [ ] masc_tagged......... MASC Tagged Corpus\n",
      "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
      "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
      "  [ ] moses_sample........ Moses Sample Models\n",
      "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
      "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
      "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
      "                           2015) subset of the Paraphrase Database.\n",
      "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
      "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
      "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
      "  [ ] nps_chat............ NPS Chat\n",
      "  [ ] omw................. Open Multilingual Wordnet\n",
      "  [ ] opinion_lexicon..... Opinion Lexicon\n",
      "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
      "  [ ] paradigms........... Paradigm Corpus\n",
      "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
      "                           Evaluation Shared Task\n",
      "Hit Enter to continue: \n",
      "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
      "                           character properties in Perl\n",
      "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
      "  [ ] pl196x.............. Polish language of the XX century sixties\n",
      "  [ ] porter_test......... Porter Stemmer Test Files\n",
      "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
      "  [ ] problem_reports..... Problem Report Corpus\n",
      "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
      "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
      "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
      "  [ ] pros_cons........... Pros and Cons\n",
      "  [ ] ptb................. Penn Treebank\n",
      "  [*] punkt............... Punkt Tokenizer Models\n",
      "  [ ] qc.................. Experimental Data for Question Classification\n",
      "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
      "                           version\n",
      "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
      "                           Portuguesa)\n",
      "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
      "  [ ] sample_grammars..... Sample Grammars\n",
      "  [ ] semcor.............. SemCor 3.0\n",
      "Hit Enter to continue: \n",
      "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
      "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
      "  [ ] sentiwordnet........ SentiWordNet\n",
      "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
      "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
      "  [ ] smultron............ SMULTRON Corpus Sample\n",
      "  [ ] snowball_data....... Snowball Data\n",
      "  [ ] spanish_grammars.... Grammars for Spanish\n",
      "  [*] state_union......... C-Span State of the Union Address Corpus\n",
      "  [*] stopwords........... Stopwords Corpus\n",
      "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
      "  [ ] swadesh............. Swadesh Wordlists\n",
      "  [ ] switchboard......... Switchboard Corpus Sample\n",
      "  [ ] tagsets............. Help on Tagsets\n",
      "  [ ] timit............... TIMIT Corpus Sample\n",
      "  [ ] toolbox............. Toolbox Sample Files\n",
      "  [ ] treebank............ Penn Treebank Sample\n",
      "  [ ] twitter_samples..... Twitter Samples\n",
      "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
      "                           (Unicode Version)\n",
      "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
      "Hit Enter to continue: \n",
      "  [ ] unicode_samples..... Unicode Samples\n",
      "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
      "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
      "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
      "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
      "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
      "  [ ] webtext............. Web Text Corpus\n",
      "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
      "  [ ] word2vec_sample..... Word2Vec Sample\n",
      "  [ ] wordnet............. WordNet\n",
      "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
      "  [ ] words............... Word Lists\n",
      "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
      "                           English Prose\n",
      "\n",
      "Collections:\n",
      "  [P] all-corpora......... All the corpora\n",
      "  [P] all-nltk............ All packages available on nltk_data gh-pages\n",
      "                           branch\n",
      "  [P] all................. All packages\n",
      "  [P] book................ Everything used in the NLTK Book\n",
      "  [P] popular............. Popular packages\n",
      "Hit Enter to continue: d\n",
      "  [ ] tests............... Packages for running tests\n",
      "  [ ] third-party......... Third-party data packages\n",
      "\n",
      "([*] marks installed packages; [P] marks partially installed collections)\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> wordnet\n",
      "Command 'wordnet' unrecognized\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> wordnet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Downloading package wordnet to\n",
      "        C:\\Users\\test\\AppData\\Roaming\\nltk_data...\n",
      "      Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    }
   ],
   "source": [
    "download_shell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'better'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('better')  #default is pos = n (noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('better',pos = 'a')  #adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('runs',pos = 'v')   #verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bad'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('worse',pos = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n    NOUN\n",
    "#v    VERB\n",
    "#a    ADJECTIVE\n",
    "#s    ADJECTIVE SATELLITE\n",
    "#r    ADVERB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech = state_union.raw(\"2002-GWBush.txt\")  #read file as single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = sent_tokenize(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"In four short months, our nation has comforted the victims, begun to rebuild New York and the Pentagon, rallied a great coalition, captured, arrested, and rid the world of thousands of terrorists, destroyed Afghanistan's terrorist training camps, saved a people from starvation, and freed a country from brutal oppression.\",\n",
       " '(Applause.)',\n",
       " 'The American flag flies again over our embassy in Kabul.',\n",
       " 'Terrorists who once occupied Afghanistan now occupy cells at Guantanamo Bay.',\n",
       " '(Applause.)',\n",
       " 'And terrorist leaders who urged followers to sacrifice their lives are running for their own.',\n",
       " '(Applause.)',\n",
       " 'America and Afghanistan are now allies against terror.',\n",
       " \"We'll be partners in rebuilding that country.\",\n",
       " 'And this evening we welcome the distinguished interim leader of a liberated Afghanistan:  Chairman Hamid Karzai.']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok[5:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns = wordnet.synsets(\"program\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('plan.n.01'), Synset('program.n.02'), Synset('broadcast.n.02'), Synset('platform.n.02'), Synset('program.n.05'), Synset('course_of_study.n.01'), Synset('program.n.07'), Synset('program.n.08'), Synset('program.v.01'), Synset('program.v.02')]\n"
     ]
    }
   ],
   "source": [
    "print(syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plan.n.01'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syns[0].name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('plan.n.01.plan'),\n",
       " Lemma('plan.n.01.program'),\n",
       " Lemma('plan.n.01.programme')]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syns[0].lemmas()  #plan is chosen as most appropriate lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plan'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syns[0].lemmas()[0].name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a series of steps to be carried out or goals to be accomplished'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syns[0].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['they drew up a six-step plan', 'they discussed plans for a new bond issue']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syns[0].examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = []\n",
    "antonyms = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "for syn in wordnet.synsets(\"good\"):\n",
    "    for l in syn.lemmas():\n",
    "        synonyms.append(l.name())\n",
    "        if l.antonyms():\n",
    "            antonyms.append(l.antonyms()[0].name())\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty\n"
     ]
    }
   ],
   "source": [
    "a = []\n",
    "if a:\n",
    "    print(\"not empty\")\n",
    "else:\n",
    "    print(\"empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trade_good', 'commodity', 'honorable', 'secure', 'proficient', 'dear', 'ripe', 'just', 'dependable', 'beneficial', 'soundly', 'expert', 'effective', 'in_effect', 'practiced', 'near', 'in_force', 'adept', 'full', 'skilful', 'unspoilt', 'goodness', 'well', 'estimable', 'respectable', 'skillful', 'honest', 'unspoiled', 'thoroughly', 'serious', 'right', 'safe', 'sound', 'undecomposed', 'good', 'upright', 'salutary'}\n"
     ]
    }
   ],
   "source": [
    "print(set(synonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'evil', 'bad', 'ill', 'evilness', 'badness'}\n"
     ]
    }
   ],
   "source": [
    "print(set(antonyms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('trunk.n.01'),\n",
       " Synset('trunk.n.02'),\n",
       " Synset('torso.n.01'),\n",
       " Synset('luggage_compartment.n.01'),\n",
       " Synset('proboscis.n.02')]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets(\"trunk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('trunk.n.01.trunk'),\n",
       " Lemma('trunk.n.01.tree_trunk'),\n",
       " Lemma('trunk.n.01.bole')]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets(\"trunk\")[0].lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the main stem of a tree; usually covered with bark; the bole is usually the part that is commercially useful for lumber'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets(\"trunk\")[0].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('trunk.n.02.trunk')]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets(\"trunk\")[1].lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'luggage consisting of a large strong case used when traveling or for storage'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets(\"trunk\")[1].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('torso.n.01.torso'),\n",
       " Lemma('torso.n.01.trunk'),\n",
       " Lemma('torso.n.01.body')]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets(\"trunk\")[2].lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('boat.n.01'), Synset('gravy_boat.n.01'), Synset('boat.v.01')]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets('boat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('ship.n.01'),\n",
       " Synset('transport.v.04'),\n",
       " Synset('ship.v.02'),\n",
       " Synset('embark.v.01'),\n",
       " Synset('ship.v.04'),\n",
       " Synset('ship.v.05')]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets('ship')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = wordnet.synset(\"boat.n.01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a small vessel for travel on water'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = wordnet.synset(\"ship.n.01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a vessel that carries passengers or freight'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9090909090909091"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.wup_similarity(w2)  #91 percent similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = wordnet.synset(\"ship.n.01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = wordnet.synset(\"car.n.01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6956521739130435"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.wup_similarity(w2)  #69 percent similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = wordnet.synset(\"ship.n.01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = wordnet.synset(\"cow.n.01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27586206896551724"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.wup_similarity(w2)  #27 percent similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"Hi, is it raining or is it cloudy?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'is': 2, 'it': 2, 'Hi': 1, ',': 1, 'raining': 1, 'or': 1, 'cloudy': 1, '?': 1})"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd = nltk.FreqDist(tokens)\n",
    "fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('is', 2), ('it', 2), ('Hi', 1)]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd.most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x209c14f2730>"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
